#!/usr/bin/env python3
"""
Masstree formatted-log analyzer.

This script is intended for analyzing `logs/masstree_formatted.json` files generated by the
repo's tracing infrastructure. Those files are JSON *arrays* of event objects, e.g.:

  [
    {"timestamp":"...Z","level":"DEBUG","fields":{"message":"...","ikey":...}, ...},
    ...
  ]

FEATURES
========

Basic Analysis:
  - summary       High-level stats (counts, time range, top messages)
  - stats         Count occurrences by field path
  - events        Print filtered events
  - cas           List CAS insert successes
  - splits        List split events with slot indices
  - perms         Decode and validate permutation values

Advanced Debugging:
  - leaf-timeline Show complete lifecycle of a specific leaf
  - correlate     Trace a key from insert through split/loss
  - missing-keys  Find keys that should exist but don't appear in final state
  - interleave    Show thread-interleaved operations on shared leaves
  - cas-failures  Analyze CAS failure patterns and causes

Anomaly Detection:
  - anomalies --slot-reuse   Detect slot reuse within epoch
  - anomalies --slot-steal   Detect slot value overwrites
  - anomalies --all          Run all anomaly detectors

USAGE EXAMPLES
==============

# Basic summary
python scripts/masstree_log_analyze.py logs/masstree_formatted.json summary

# Find all events for a specific key
python scripts/masstree_log_analyze.py logs/masstree_formatted.json --ikey 20000 events

# Trace a key's lifecycle (insert â†’ split â†’ loss)
python scripts/masstree_log_analyze.py logs/masstree_formatted.json correlate --ikey 20000

# Find which expected keys are missing
python scripts/masstree_log_analyze.py logs/masstree_formatted.json missing-keys --expected 0-100,20000,30000

# Show all operations on a specific leaf
python scripts/masstree_log_analyze.py logs/masstree_formatted.json leaf-timeline 0x7913d0000d80

# Detect slot stealing
python scripts/masstree_log_analyze.py logs/masstree_formatted.json anomalies --slot-steal

# Show thread interleaving on contested leaves
python scripts/masstree_log_analyze.py logs/masstree_formatted.json interleave --min-threads 2

# Analyze why CAS operations failed
python scripts/masstree_log_analyze.py logs/masstree_formatted.json cas-failures

No third-party dependencies (stdlib only).
"""

from __future__ import annotations

import argparse
import csv
import dataclasses
import datetime as dt
import json
import re
import sys
from collections import Counter, defaultdict
from pathlib import Path
from typing import Any, Iterator, Mapping, Sequence


WIDTH_DEFAULT = 15

# ============================================================================
# Message patterns for event classification
# ============================================================================

MSG_CAS_SUCCESS = "CAS insert: permutation CAS succeeded"
MSG_CAS_SLOT_CLAIM = "CAS insert: slot claimed"
MSG_CAS_VERSION_ABORT_1 = (
    "CAS insert: version changed after slot claim, aborting (check 1)"
)
MSG_CAS_VERSION_ABORT_2 = (
    "CAS insert: version changed after key write, aborting (check 2)"
)
MSG_CAS_VERSION_ABORT_3 = "CAS insert: version changed before perm CAS, aborting"
MSG_CAS_PERM_FAIL = "CAS insert: permutation CAS failed"
MSG_SPLIT_START = "split_into: starting split with ikey distribution"
MSG_SPLIT_COMPLETE = "insert_concurrent: split completed"
MSG_LOCKED_INSERT = "insert_concurrent: inserting key"
MSG_LOCKED_SPLIT_DECISION = "insert_concurrent: split required"

# Related messages
MSG_BLINK_WARNING = (
    "get: NotFound but ikey >= next leaf bound (stale routing; should follow B-link)"
)
MSG_ADVANCE_TO_KEY = "advance_to_key_by_bound: reached end of B-link chain"
MSG_GET_NOT_FOUND = "get_with_guard: key not found"
MSG_IMMEDIATE_VERIFY_FAIL = "immediate verify failed"

# All CAS abort messages
CAS_ABORT_MESSAGES = {
    MSG_CAS_VERSION_ABORT_1,
    MSG_CAS_VERSION_ABORT_2,
    MSG_CAS_VERSION_ABORT_3,
    MSG_CAS_PERM_FAIL,
}


# ============================================================================
# Utility functions
# ============================================================================


def _parse_timestamp(value: str) -> dt.datetime:
    """Parse RFC3339 timestamp with 'Z' suffix."""
    if value.endswith("Z"):
        value = value[:-1] + "+00:00"
    return dt.datetime.fromisoformat(value)


def _format_timestamp(value: dt.datetime) -> str:
    """Format datetime as RFC3339 with 'Z' suffix for UTC."""
    if value.tzinfo is not None and value.utcoffset() == dt.timedelta(0):
        return value.astimezone(dt.timezone.utc).isoformat().replace("+00:00", "Z")
    return value.isoformat()


def _format_timestamp_short(value: dt.datetime) -> str:
    """Format timestamp showing only time portion with microseconds."""
    return value.strftime("%H:%M:%S.%f")


def _parse_hex_ptr(ptr_text: str) -> str:
    """Normalize hex pointer to lowercase 0x-prefixed form."""
    ptr_text = ptr_text.strip()
    if not ptr_text:
        raise ValueError("empty pointer string")
    if ptr_text.startswith("0x") or ptr_text.startswith("0X"):
        return "0x" + ptr_text[2:].lower()
    if re.fullmatch(r"[0-9a-fA-F]+", ptr_text) is None:
        raise ValueError(f"not a hex pointer: {ptr_text!r}")
    return "0x" + ptr_text.lower()


def _get_in(mapping: Mapping[str, Any], path: str) -> Any:
    """Get nested values using dot paths like 'fields.message'."""
    cur: Any = mapping
    for part in path.split("."):
        if not isinstance(cur, Mapping):
            return None
        cur = cur.get(part)
    return cur


def _parse_key_ranges(spec: str) -> set[int]:
    """
    Parse key specification like '0-10,20,30-35' into a set of integers.

    Formats supported:
      - Single value: '42'
      - Range: '0-10' (inclusive)
      - Comma-separated: '1,2,3'
      - Mixed: '0-5,10,20-25'
    """
    result: set[int] = set()
    for part in spec.split(","):
        part = part.strip()
        if not part:
            continue
        if "-" in part and not part.startswith("-"):
            # Range like '0-10'
            start_s, end_s = part.split("-", 1)
            start = int(start_s.strip())
            end = int(end_s.strip())
            result.update(range(start, end + 1))
        else:
            # Single value
            result.add(int(part))
    return result


# ============================================================================
# Event data structure
# ============================================================================


@dataclasses.dataclass(frozen=True, slots=True)
class Event:
    """Represents a single tracing event from the log."""

    timestamp: dt.datetime
    level: str
    target: str
    filename: str
    line_number: int | None
    thread_id: str | None
    fields: Mapping[str, Any]
    raw: Mapping[str, Any]

    @property
    def message(self) -> str:
        msg = self.fields.get("message")
        return msg if isinstance(msg, str) else ""

    @property
    def ikey(self) -> int | None:
        val = self.fields.get("ikey")
        return val if isinstance(val, int) else None

    @property
    def slot(self) -> int | None:
        val = self.fields.get("slot")
        return val if isinstance(val, int) else None

    @property
    def leaf_ptr(self) -> str | None:
        """Get leaf pointer from various field names."""
        for key in ("leaf_ptr", "left_ptr"):
            val = self.fields.get(key)
            if isinstance(val, str):
                return val
        return None

    def get(self, path: str) -> Any:
        if path.startswith("fields."):
            return _get_in({"fields": self.fields}, path)
        return _get_in(self.raw, path)


# ============================================================================
# Event loading and filtering
# ============================================================================


def load_formatted_events(path: Path) -> list[Event]:
    """
    Load a `masstree_formatted.json` file (JSON array of dict events).
    Returns events sorted by timestamp.
    """
    with path.open("r", encoding="utf-8") as file:
        payload = json.load(file)

    if not isinstance(payload, list):
        raise ValueError(
            f"{path} is not a JSON array; this tool expects masstree_formatted.json output"
        )

    events: list[Event] = []
    for idx, item in enumerate(payload):
        if not isinstance(item, Mapping):
            raise ValueError(f"{path}: event[{idx}] is not an object")

        timestamp_raw = item.get("timestamp")
        level = item.get("level") or ""
        target = item.get("target") or ""
        filename = item.get("filename") or ""
        line_number = item.get("line_number")
        thread_id = item.get("threadId")
        fields = item.get("fields") or {}

        if not isinstance(timestamp_raw, str):
            raise ValueError(f"{path}: event[{idx}] missing string timestamp")
        if not isinstance(level, str):
            level = str(level)
        if not isinstance(target, str):
            target = str(target)
        if not isinstance(filename, str):
            filename = str(filename)
        if line_number is not None and not isinstance(line_number, int):
            try:
                line_number = int(line_number)
            except Exception:
                line_number = None
        if thread_id is not None and not isinstance(thread_id, str):
            thread_id = str(thread_id)
        if not isinstance(fields, Mapping):
            fields = {"fields": fields}

        events.append(
            Event(
                timestamp=_parse_timestamp(timestamp_raw),
                level=level,
                target=target,
                filename=filename,
                line_number=line_number,
                thread_id=thread_id,
                fields=fields,
                raw=item,
            )
        )

    events.sort(key=lambda e: e.timestamp)
    return events


def iter_filtered(
    events: Sequence[Event],
    *,
    level: set[str] | None = None,
    target_re: re.Pattern[str] | None = None,
    message_re: re.Pattern[str] | None = None,
    ikey: int | None = None,
    leaf_ptr: str | None = None,
    slot: int | None = None,
    thread_id: str | None = None,
    since: dt.datetime | None = None,
    until: dt.datetime | None = None,
) -> Iterator[Event]:
    """Filter events by various criteria."""
    for event in events:
        if since is not None and event.timestamp < since:
            continue
        if until is not None and event.timestamp > until:
            continue
        if level is not None and event.level not in level:
            continue
        if target_re is not None and not target_re.search(event.target):
            continue
        if message_re is not None and not message_re.search(event.message):
            continue
        if thread_id is not None and event.thread_id != thread_id:
            continue
        if ikey is not None:
            value = event.fields.get("ikey")
            if value != ikey:
                continue
        if leaf_ptr is not None:
            value = event.fields.get("leaf_ptr") or event.fields.get("left_ptr")
            if value != leaf_ptr:
                continue
        if slot is not None:
            value = event.fields.get("slot")
            if value != slot:
                continue
        yield event


# ============================================================================
# Event printing utilities
# ============================================================================


def print_event(event: Event, *, compact: bool = False) -> None:
    """Print an event in human-readable format."""
    pieces: list[str] = []

    if compact:
        pieces.append(_format_timestamp_short(event.timestamp))
    else:
        pieces.append(_format_timestamp(event.timestamp))

    if event.level:
        pieces.append(event.level)
    if event.thread_id:
        pieces.append(f"T{event.thread_id}")
    if not compact and event.target:
        pieces.append(event.target)
    if not compact and event.filename and event.line_number:
        pieces.append(f"{event.filename}:{event.line_number}")
    if event.message:
        # Truncate long messages in compact mode
        msg = event.message
        if compact and len(msg) > 50:
            msg = msg[:47] + "..."
        pieces.append(msg)

    # Common fields as key=value pairs
    common_keys = [
        "ikey",
        "slot",
        "leaf_ptr",
        "left_ptr",
        "right_ptr",
        "split_pos",
        "split_ikey",
        "old_perm_size",
        "new_perm_size",
    ]
    extras: list[str] = []
    for key in common_keys:
        if key in event.fields:
            extras.append(f"{key}={event.fields[key]}")
    if extras:
        pieces.append("[" + " ".join(extras) + "]")

    print(" | ".join(pieces))


def print_event_detailed(event: Event) -> None:
    """Print an event with all fields visible."""
    print(f"  Time:    {_format_timestamp(event.timestamp)}")
    print(f"  Level:   {event.level}")
    print(f"  Thread:  {event.thread_id or '(none)'}")
    print(f"  Target:  {event.target}")
    print(f"  Message: {event.message}")
    print("  Fields:")
    for key, value in event.fields.items():
        if key != "message":
            print(f"    {key}: {value}")


# ============================================================================
# Permutation decoder
# ============================================================================


@dataclasses.dataclass(frozen=True, slots=True)
class DecodedPermuter:
    """Decoded permutation showing slot assignments."""

    size: int
    positions: tuple[int, ...]  # length WIDTH, slots in each position

    @property
    def active_slots(self) -> tuple[int, ...]:
        """Slots currently in use (positions 0..size)."""
        return self.positions[: self.size]

    @property
    def free_slots(self) -> tuple[int, ...]:
        """Slots available for new inserts (positions size..WIDTH)."""
        return self.positions[self.size :]

    @property
    def back(self) -> int:
        """The next slot that would be used by back()."""
        return self.positions[-1]


def decode_permuter(value: int, *, width: int = WIDTH_DEFAULT) -> DecodedPermuter:
    """
    Decode a 64-bit permutation value into its components.

    Layout: bits 0-3 = size, bits 4-7 = slot[0], bits 8-11 = slot[1], etc.
    """
    size = value & 0xF
    if size > width:
        raise ValueError(f"permuter size {size} > width {width}")
    positions: list[int] = []
    for i in range(width):
        slot = (value >> ((i * 4) + 4)) & 0xF
        positions.append(int(slot))
    return DecodedPermuter(size=size, positions=tuple(positions))


def validate_permuter(
    decoded: DecodedPermuter, *, width: int = WIDTH_DEFAULT
) -> list[str]:
    """
    Validate permutation invariants.

    Returns list of issues found (empty if valid).
    """
    issues: list[str] = []
    if decoded.size < 0 or decoded.size > width:
        issues.append(f"invalid size {decoded.size} for width {width}")
        return issues
    seen: set[int] = set()
    for pos, slot in enumerate(decoded.positions):
        if slot < 0 or slot >= width:
            issues.append(f"pos {pos}: invalid slot {slot}")
            continue
        if slot in seen:
            issues.append(f"duplicate slot {slot} (pos {pos})")
        seen.add(slot)
    missing = [s for s in range(width) if s not in seen]
    if missing:
        issues.append(f"missing slots: {missing}")
    return issues


# ============================================================================
# Basic commands: summary, stats, events, cas, splits, perms
# ============================================================================


def cmd_summary(events: Sequence[Event]) -> None:
    """Print high-level summary statistics."""
    if not events:
        print("No events.")
        return

    start = events[0].timestamp
    end = events[-1].timestamp
    duration = end - start

    by_level = Counter(e.level for e in events)
    by_target = Counter(e.target for e in events)
    by_message = Counter(e.message for e in events)
    by_thread = Counter(e.thread_id or "" for e in events)

    print(f"Events: {len(events)}")
    print(
        f"Time range: {_format_timestamp(start)} .. {_format_timestamp(end)} ({duration})"
    )

    print("\nLevels:")
    for level, count in by_level.most_common():
        print(f"  {level or '(none)'}: {count}")

    print("\nTop targets:")
    for target, count in by_target.most_common(10):
        print(f"  {target or '(none)'}: {count}")

    print("\nTop messages:")
    for msg, count in by_message.most_common(10):
        print(f"  {msg or '(none)'}: {count}")

    threads = [t for t in by_thread if t]
    if threads:
        print("\nThreads:")
        for t, count in by_thread.most_common():
            if not t:
                continue
            print(f"  {t}: {count}")

    cas_success = sum(1 for e in events if e.message == MSG_CAS_SUCCESS)
    cas_aborts = sum(1 for e in events if e.message in CAS_ABORT_MESSAGES)
    splits = sum(1 for e in events if e.message == MSG_SPLIT_COMPLETE)

    print(f"  CAS successes: {cas_success}")
    print(f"  CAS aborts: {cas_aborts}")
    print(f"  Splits: {splits}")
    if cas_success + cas_aborts > 0:
        rate = cas_success / (cas_success + cas_aborts) * 100
        print(f"  CAS success rate: {rate:.1f}%")


def cmd_stats(events: Sequence[Event], *, by: str, top: int) -> None:
    """Count occurrences of values at a field path."""
    counter: Counter[str] = Counter()
    for event in events:
        value = event.get(by)
        if value is None:
            continue
        counter[str(value)] += 1

    for value, count in counter.most_common(top):
        print(f"{count:>6}  {value}")


def cmd_events(events: Sequence[Event], *, limit: int | None, compact: bool) -> None:
    """Print matching events."""
    rows = events if limit is None else events[:limit]
    for event in rows:
        print_event(event, compact=compact)


def cmd_cas(events: Sequence[Event], *, limit: int | None) -> None:
    """List CAS insert success events."""
    cas_events = [e for e in events if e.message == MSG_CAS_SUCCESS]
    if limit is not None:
        cas_events = cas_events[:limit]
    for event in cas_events:
        print_event(event)


def cmd_splits(events: Sequence[Event], *, limit: int | None, show_slots: bool) -> None:
    """
    List split-related events.

    With --show-slots, also displays the slot indices (not just ikeys) for each position,
    which helps distinguish permutation corruption from slot overwrite.
    """
    split_events = [
        e for e in events if e.message in (MSG_SPLIT_COMPLETE, MSG_SPLIT_START)
    ]
    if limit is not None:
        split_events = split_events[:limit]

    for event in split_events:
        print_event(event)

        if show_slots and event.message == MSG_SPLIT_START:
            # Extract and display slot information if available
            perm_value = event.fields.get("perm_value")
            if perm_value is not None:
                try:
                    decoded = decode_permuter(int(perm_value))
                    print(
                        f"    Permutation: size={decoded.size} slots={list(decoded.active_slots)}"
                    )
                    issues = validate_permuter(decoded)
                    for issue in issues:
                        print(f"    âš ï¸  {issue}")
                except Exception as ex:
                    print(f"    (decode error: {ex})")

            # Show left/right ikey arrays with indices
            split_pos = event.fields.get("split_pos")
            if split_pos is not None:
                left_ikeys = []
                right_ikeys = []
                for i in range(WIDTH_DEFAULT):
                    lk = event.fields.get(f"left_ikeys_{i}")
                    rk = event.fields.get(f"right_ikeys_{i}")
                    if lk is not None:
                        left_ikeys.append((i, lk))
                    if rk is not None:
                        right_ikeys.append((i, rk))

                if left_ikeys:
                    print(f"    Left [{split_pos}]: ", end="")
                    print(", ".join(f"[{i}]={k}" for i, k in left_ikeys[:split_pos]))
                if right_ikeys:
                    print("    Right: ", end="")
                    print(", ".join(f"[{i}]={k}" for i, k in right_ikeys))


def cmd_perms(events: Sequence[Event], *, width: int) -> None:
    """Decode and validate any `fields.perm_value` occurrences."""
    count = 0
    for event in events:
        perm_value = event.fields.get("perm_value")
        if perm_value is None:
            continue
        try:
            perm_int = int(perm_value)
        except Exception:
            print(
                f"Bad perm_value={perm_value!r} at {_format_timestamp(event.timestamp)}"
            )
            continue
        decoded = decode_permuter(perm_int, width=width)
        issues = validate_permuter(decoded, width=width)
        print_event(event)
        print(f"  perm_value={perm_int:#018x} size={decoded.size} back={decoded.back}")
        print(f"  active={list(decoded.active_slots)}")
        print(f"  free={list(decoded.free_slots)}")
        if issues:
            for issue in issues:
                print(f"  âš ï¸  ISSUE: {issue}")
        count += 1
    if count == 0:
        print("No events with `fields.perm_value` found.")


# ============================================================================
# Advanced commands: leaf-timeline, correlate, missing-keys, interleave, cas-failures
# ============================================================================


def cmd_leaf_timeline(events: Sequence[Event], leaf_ptr: str) -> None:
    """
    Show complete lifecycle of a specific leaf.

    Displays all operations on the leaf in chronological order, including:
    - CAS inserts (success/failure)
    - Locked inserts
    - Splits (as source or destination)
    - Version checks

    This is essential for understanding race condition timelines.
    """
    # Normalize the pointer
    try:
        leaf_ptr = _parse_hex_ptr(leaf_ptr)
    except ValueError as e:
        print(f"Error: {e}", file=sys.stderr)
        return

    # Find all events mentioning this leaf
    leaf_events: list[Event] = []
    for event in events:
        # Check various pointer fields
        ptrs = [
            event.fields.get("leaf_ptr"),
            event.fields.get("left_ptr"),
            event.fields.get("right_ptr"),
        ]
        if leaf_ptr in [p for p in ptrs if isinstance(p, str)]:
            leaf_events.append(event)

    if not leaf_events:
        print(f"No events found for leaf {leaf_ptr}")
        return

    print(f"=== Leaf Timeline: {leaf_ptr} ===")
    print(f"Events: {len(leaf_events)}")
    print(
        f"Time span: {_format_timestamp(leaf_events[0].timestamp)} to {_format_timestamp(leaf_events[-1].timestamp)}"
    )
    print()

    # Group by "epochs" (split boundaries)
    epoch = 0
    last_was_split = False

    for event in leaf_events:
        # Mark epoch boundaries
        if event.message == MSG_SPLIT_COMPLETE:
            if not last_was_split:
                epoch += 1
                print(f"\n--- Epoch {epoch} (post-split) ---")
            last_was_split = True
        else:
            last_was_split = False

        # Color-code by event type
        prefix = "  "
        if event.message == MSG_CAS_SUCCESS:
            prefix = "âœ“ "
        elif event.message in CAS_ABORT_MESSAGES:
            prefix = "âœ— "
        elif event.message == MSG_SPLIT_START:
            prefix = "âš¡"
        elif event.message == MSG_SPLIT_COMPLETE:
            prefix = "ðŸ”€"

        print(f"{prefix} ", end="")
        print_event(event, compact=True)


def cmd_correlate(events: Sequence[Event], ikey: int) -> None:
    """
    Trace a specific key's lifecycle from insert through split/loss.

    Shows the complete history of operations involving this ikey:
    1. Initial insert attempt (CAS or locked)
    2. Any split operations that should include this key
    3. Whether the key appears in the correct split destination

    Useful for debugging "key inserted but missing" scenarios.
    """
    print(f"=== Correlation Report: ikey={ikey} ===\n")

    # Find all events mentioning this ikey
    key_events = [e for e in events if e.ikey == ikey]

    if not key_events:
        print(f"No events found for ikey={ikey}")
        print("\nTip: The key might be encoded differently or missing from logs.")
        return

    print(f"Found {len(key_events)} events for this key:\n")

    # Categorize events
    insert_attempts: list[Event] = []
    insert_successes: list[Event] = []
    split_mentions: list[Event] = []
    other_events: list[Event] = []

    for event in key_events:
        if event.message == MSG_CAS_SUCCESS:
            insert_successes.append(event)
        elif "insert" in event.message.lower() or "claim" in event.message.lower():
            insert_attempts.append(event)
        elif "split" in event.message.lower():
            split_mentions.append(event)
        else:
            other_events.append(event)

    # Report insert status
    print("--- Insert History ---")
    if insert_successes:
        print(f"âœ“ Key was successfully inserted {len(insert_successes)} time(s):")
        for event in insert_successes:
            print(f"  Leaf: {event.leaf_ptr}, Slot: {event.slot}")
            print(f"  Time: {_format_timestamp(event.timestamp)}")
            print(
                f"  Perm: {event.fields.get('old_perm_size')} â†’ {event.fields.get('new_perm_size')}"
            )
    else:
        print("âœ— No successful insert found for this key")

    if insert_attempts:
        print(f"\nInsert attempts/aborts: {len(insert_attempts)}")
        for event in insert_attempts[:5]:  # Show first 5
            print_event(event, compact=True)

    # Check splits that might have affected this key
    if insert_successes:
        success_leaf = insert_successes[0].leaf_ptr
        success_time = insert_successes[0].timestamp

        # Find splits on this leaf after the insert
        later_splits = [
            e
            for e in events
            if e.message in (MSG_SPLIT_START, MSG_SPLIT_COMPLETE)
            and e.timestamp > success_time
            and (
                e.fields.get("left_ptr") == success_leaf
                or e.fields.get("leaf_ptr") == success_leaf
            )
        ]

        if later_splits:
            print("\n--- Splits After Insert ---")
            print(
                f"Found {len(later_splits)} split events on leaf {success_leaf} after insert:"
            )
            for event in later_splits:
                print_event(event, compact=True)

                # Check if our key appears in the split distribution
                if event.message == MSG_SPLIT_START:
                    found_in_left = False
                    found_in_right = False
                    for i in range(WIDTH_DEFAULT):
                        lk = event.fields.get(f"left_ikeys_{i}")
                        rk = event.fields.get(f"right_ikeys_{i}")
                        if lk == ikey:
                            found_in_left = True
                        if rk == ikey:
                            found_in_right = True

                    if found_in_left:
                        print(f"    âœ“ Key {ikey} found in LEFT side of split")
                    elif found_in_right:
                        print(f"    âœ“ Key {ikey} found in RIGHT side of split")
                    else:
                        print(f"    âš ï¸  Key {ikey} NOT FOUND in split distribution!")
                        print("    This indicates the key was LOST before the split.")

    if split_mentions:
        print("\n--- Split Mentions ---")
        for event in split_mentions:
            print_event(event, compact=True)

    if other_events:
        print(f"\n--- Other Events ({len(other_events)}) ---")
        for event in other_events[:10]:
            print_event(event, compact=True)


def cmd_missing_keys(
    events: Sequence[Event],
    expected: set[int],
    *,
    show_found: bool = False,
) -> None:
    """
    Find keys that should exist but don't appear in successful inserts.

    Compares expected keys against actual CAS/locked insert successes.
    For missing keys, shows any partial history (attempts, aborts).
    """
    print("=== Missing Keys Report ===")
    print(f"Expected keys: {len(expected)}")

    # Find all successfully inserted keys
    found_keys: set[int] = set()
    key_to_success: dict[int, Event] = {}

    for event in events:
        if event.message == MSG_CAS_SUCCESS:
            if event.ikey is not None:
                found_keys.add(event.ikey)
                key_to_success[event.ikey] = event
        elif event.message == MSG_LOCKED_INSERT:
            if event.ikey is not None:
                found_keys.add(event.ikey)
                key_to_success[event.ikey] = event

    missing = expected - found_keys
    found = expected & found_keys
    extra = found_keys - expected

    print(f"Found in logs: {len(found)}")
    print(f"Missing: {len(missing)}")
    if extra:
        print(f"Extra (not in expected): {len(extra)}")

    if show_found and found:
        print("\n--- Found Keys ---")
        for k in sorted(found)[:50]:
            event = key_to_success.get(k)
            if event:
                print(f"  {k}: leaf={event.leaf_ptr} slot={event.slot}")

    if missing:
        print("\n--- Missing Keys ---")
        for k in sorted(missing):
            print(f"\n  Key {k}:")

            # Find any events mentioning this key
            key_events = [e for e in events if e.ikey == k]
            if not key_events:
                print("    No events found for this key")
            else:
                print(f"    {len(key_events)} event(s):")
                for event in key_events[:5]:
                    print("      ", end="")
                    print_event(event, compact=True)


def cmd_interleave(
    events: Sequence[Event],
    *,
    min_threads: int = 2,
    limit: int | None = None,
) -> None:
    """
    Show thread-interleaved operations on shared leaves.

    Identifies leaves with operations from multiple threads and shows
    the interleaved timeline. This helps visualize race conditions.
    """
    print("=== Thread Interleaving Report ===")
    print(f"Minimum threads per leaf: {min_threads}\n")

    # Group events by leaf and track threads
    per_leaf: dict[str, list[Event]] = defaultdict(list)
    leaf_threads: dict[str, set[str]] = defaultdict(set)

    for event in events:
        leaf = event.leaf_ptr
        if leaf and event.thread_id:
            per_leaf[leaf].append(event)
            leaf_threads[leaf].add(event.thread_id)

    # Find contested leaves
    contested = [
        (leaf, threads)
        for leaf, threads in leaf_threads.items()
        if len(threads) >= min_threads
    ]

    if not contested:
        print(f"No leaves found with {min_threads}+ threads")
        return

    # Sort by thread count (most contested first)
    contested.sort(key=lambda x: len(x[1]), reverse=True)

    if limit:
        contested = contested[:limit]

    print(f"Found {len(contested)} contested leaf(ves):\n")

    for leaf, threads in contested:
        print(
            f"â”â”â” Leaf {leaf} ({len(threads)} threads: {', '.join(sorted(threads))}) â”â”â”"
        )

        leaf_events = per_leaf[leaf]
        leaf_events.sort(key=lambda e: e.timestamp)

        # Show interleaved timeline with thread markers
        last_thread: str | None = None
        for event in leaf_events:
            # Mark thread switches
            if event.thread_id != last_thread:
                if last_thread is not None:
                    print("  â”„â”„â”„ thread switch â”„â”„â”„")
                last_thread = event.thread_id

            # Indent by thread
            thread_idx = (
                sorted(threads).index(event.thread_id) if event.thread_id else 0
            )
            indent = "  " * thread_idx

            prefix = ""
            if event.message == MSG_CAS_SUCCESS:
                prefix = "âœ“"
            elif event.message in CAS_ABORT_MESSAGES:
                prefix = "âœ—"
            elif "split" in event.message.lower():
                prefix = "âš¡"

            print(f"{indent}{prefix} T{event.thread_id}: ", end="")

            # Compact info
            msg = event.message
            if len(msg) > 40:
                msg = msg[:37] + "..."
            extras = []
            if event.ikey is not None:
                extras.append(f"ikey={event.ikey}")
            if event.slot is not None:
                extras.append(f"slot={event.slot}")
            extra_str = f" [{', '.join(extras)}]" if extras else ""

            print(f"{_format_timestamp_short(event.timestamp)} {msg}{extra_str}")

        print()


def cmd_cas_failures(events: Sequence[Event], *, limit: int | None = None) -> None:
    """
    Analyze CAS failure patterns and causes.

    Categorizes CAS aborts by:
    - Failure reason (version change, perm CAS fail, slot stolen)
    - Affected leaf
    - Thread

    Helps identify contention hotspots and failure patterns.
    """
    print("=== CAS Failure Analysis ===\n")

    # Categorize failures
    by_reason: dict[str, list[Event]] = defaultdict(list)
    by_leaf: dict[str, list[Event]] = defaultdict(list)
    by_thread: dict[str, list[Event]] = defaultdict(list)

    for event in events:
        if event.message in CAS_ABORT_MESSAGES:
            by_reason[event.message].append(event)
            if event.leaf_ptr:
                by_leaf[event.leaf_ptr].append(event)
            if event.thread_id:
                by_thread[event.thread_id].append(event)

    total_failures = sum(len(v) for v in by_reason.values())
    total_successes = sum(1 for e in events if e.message == MSG_CAS_SUCCESS)

    print(f"Total CAS attempts: {total_failures + total_successes}")
    print(f"Successes: {total_successes}")
    print(f"Failures: {total_failures}")
    if total_failures + total_successes > 0:
        rate = total_failures / (total_failures + total_successes) * 100
        print(f"Failure rate: {rate:.1f}%")

    print("\n--- By Reason ---")
    for reason, evts in sorted(by_reason.items(), key=lambda x: -len(x[1])):
        # Shorten the message
        short = reason.replace("CAS insert: ", "").replace(", aborting", "")
        print(f"  {len(evts):>5}  {short}")

    print("\n--- By Leaf (top 10) ---")
    for leaf, evts in sorted(by_leaf.items(), key=lambda x: -len(x[1]))[:10]:
        print(f"  {len(evts):>5}  {leaf}")

    print("\n--- By Thread ---")
    for thread, evts in sorted(by_thread.items(), key=lambda x: -len(x[1])):
        print(f"  {len(evts):>5}  T{thread}")

    # Show sample failures
    if limit and limit > 0:
        print(f"\n--- Sample Failures (first {limit}) ---")
        all_failures = [e for e in events if e.message in CAS_ABORT_MESSAGES]
        for event in all_failures[:limit]:
            print_event(event, compact=True)


# ============================================================================
# Anomaly detection
# ============================================================================


def anomaly_slot_reuse(events: Sequence[Event]) -> int:
    """
    Heuristic: within a "leaf epoch" (between splits / observed size resets),
    a CAS-inserted slot should not be reused for a different ikey.

    This is not a formal proof (splits can free slots), but it is a strong signal
    for the "slot stealing" class of bugs.
    """
    print("--- Anomaly: Slot Reuse Within Epoch ---\n")

    per_leaf: dict[str, list[Event]] = defaultdict(list)
    for event in events:
        if event.message == MSG_CAS_SUCCESS:
            if event.leaf_ptr:
                per_leaf[event.leaf_ptr].append(event)
        elif event.message == MSG_SPLIT_COMPLETE:
            left_ptr = event.fields.get("left_ptr")
            if isinstance(left_ptr, str):
                per_leaf[left_ptr].append(event)

    findings = 0
    for leaf_ptr, stream in per_leaf.items():
        stream.sort(key=lambda e: e.timestamp)
        epoch = 0
        last_old_size: int | None = None
        slots_to_ikey: dict[int, int] = {}

        for event in stream:
            if event.message == MSG_SPLIT_COMPLETE:
                epoch += 1
                last_old_size = None
                slots_to_ikey.clear()
                continue

            if event.message != MSG_CAS_SUCCESS:
                continue

            slot = event.slot
            ikey = event.ikey
            old_size = event.fields.get("old_perm_size")

            if slot is None or ikey is None:
                continue
            if (
                isinstance(old_size, int)
                and last_old_size is not None
                and old_size < last_old_size
            ):
                epoch += 1
                slots_to_ikey.clear()
            if isinstance(old_size, int):
                last_old_size = old_size

            prev = slots_to_ikey.get(slot)
            if prev is None:
                slots_to_ikey[slot] = ikey
                continue

            if prev != ikey:
                findings += 1
                print(
                    f"âš ï¸  [slot-reuse] leaf={leaf_ptr} epoch={epoch} slot={slot} "
                    f"prev_ikey={prev} new_ikey={ikey}"
                )
                print(f"    at {_format_timestamp(event.timestamp)}")

    if findings == 0:
        print("âœ“ No slot-reuse anomalies detected.")
    else:
        print(f"\nTotal: {findings} anomalies")
    return findings


def anomaly_slot_steal(events: Sequence[Event]) -> int:
    """
    Detect slot value overwrites.

    Looks for the pattern where:
    1. Thread A successfully CAS inserts at slot S
    2. Thread B (stale) overwrites slot S with a different value
    3. Thread B's permutation CAS fails and it clears the slot

    Detection heuristic: Look for CAS successes followed by the same slot
    being used by a different key within a short time window, without an
    intervening split.
    """
    print("--- Anomaly: Slot Stealing ---\n")

    # Group by leaf
    per_leaf: dict[str, list[Event]] = defaultdict(list)
    for event in events:
        if event.leaf_ptr:
            if (
                event.message in (MSG_CAS_SUCCESS, MSG_SPLIT_COMPLETE)
                or event.message in CAS_ABORT_MESSAGES
            ):
                per_leaf[event.leaf_ptr].append(event)

    findings = 0

    for leaf_ptr, stream in per_leaf.items():
        stream.sort(key=lambda e: e.timestamp)

        # Track slot ownership: slot -> (ikey, thread, timestamp)
        slot_owner: dict[int, tuple[int, str | None, dt.datetime]] = {}

        for event in stream:
            if event.message == MSG_SPLIT_COMPLETE:
                # Split resets ownership
                slot_owner.clear()
                continue

            if event.message == MSG_CAS_SUCCESS:
                slot = event.slot
                ikey = event.ikey
                if slot is None or ikey is None:
                    continue

                # Check if slot was already owned
                if slot in slot_owner:
                    prev_ikey, prev_thread, prev_time = slot_owner[slot]
                    if prev_ikey != ikey:
                        # Different key using same slot!
                        time_diff = (event.timestamp - prev_time).total_seconds() * 1000
                        findings += 1
                        print(f"âš ï¸  [slot-steal] leaf={leaf_ptr} slot={slot}")
                        print(f"    Original: ikey={prev_ikey} thread={prev_thread}")
                        print(f"    Stolen by: ikey={ikey} thread={event.thread_id}")
                        print(f"    Time delta: {time_diff:.3f}ms")
                        print(f"    at {_format_timestamp(event.timestamp)}")
                        print()

                # Update ownership
                slot_owner[slot] = (ikey, event.thread_id, event.timestamp)

    if findings == 0:
        print("âœ“ No slot-stealing anomalies detected.")
    else:
        print(f"\nTotal: {findings} anomalies")
        print(
            "\nThis pattern suggests the CAS insert is not using NULL-claim semantics."
        )

    return findings


def cmd_blink_warnings(events: Sequence[Event], *, limit: int | None = None) -> None:
    """
    List B-link navigation warnings.

    These warnings indicate that a get() operation found NotFound but the key's
    ikey was >= the next leaf's bound, suggesting the B-link should have been
    followed. This is a key symptom of the visibility race.
    """
    print("=== B-Link Navigation Warnings ===\n")

    warnings = [e for e in events if e.message == MSG_BLINK_WARNING]

    if not warnings:
        print("âœ“ No B-link navigation warnings found.")
        print("\nThis means either:")
        print("  - The bug didn't manifest in this run")
        print("  - Tracing overhead masked the race condition")
        return

    print(f"Found {len(warnings)} B-link warnings:\n")

    # Group by ikey to see if specific keys are affected
    by_ikey: dict[int, list[Event]] = defaultdict(list)
    by_leaf: dict[str, list[Event]] = defaultdict(list)

    for event in warnings:
        if event.ikey is not None:
            by_ikey[event.ikey].append(event)
        if event.leaf_ptr:
            by_leaf[event.leaf_ptr].append(event)

    # Show affected keys
    print("--- Affected Keys ---")
    for ikey, evts in sorted(by_ikey.items()):
        print(f"  ikey={ikey}: {len(evts)} warning(s)")

    # Show affected leaves
    print("\n--- Affected Leaves ---")
    for leaf, evts in sorted(by_leaf.items(), key=lambda x: -len(x[1]))[:10]:
        print(f"  {leaf}: {len(evts)} warning(s)")

    # Show sample warnings
    display = warnings[:limit] if limit else warnings[:10]
    print(f"\n--- Sample Warnings (showing {len(display)}) ---")
    for event in display:
        print_event(event, compact=True)
        # Show additional context
        bound = event.fields.get("leaf_bound_ikey")
        next_ptr = event.fields.get("next_ptr")
        next_bound = event.fields.get("next_bound_ikey")
        if bound is not None:
            print(
                f"    leaf_bound={bound}, next_ptr={next_ptr}, next_bound={next_bound}"
            )


def anomaly_writer_reader_divergence(events: Sequence[Event]) -> int:
    """
    Detect writer/reader leaf disagreement.

    Pattern:
    1. Writer selects leaf L1 via advance_to_key_by_bound for key K
    2. Insert succeeds at L1
    3. Immediate get for key K searches leaf L2 (different!)
    4. Key not found
    """
    print("--- Anomaly: Writer/Reader Leaf Divergence ---\n")

    # Track writer leaf selection: {(ikey, thread) -> leaf_ptr}
    writer_leaf: dict[tuple[int, str | None], tuple[str, dt.datetime]] = {}

    # Track insert successes: {ikey -> (leaf_ptr, timestamp, thread)}
    insert_success: dict[int, tuple[str, dt.datetime, str | None]] = {}

    findings = 0

    for event in events:
        ikey = event.ikey
        if ikey is None:
            continue

        # Writer selecting a leaf
        if event.message == MSG_ADVANCE_TO_KEY:
            leaf = event.leaf_ptr
            if leaf:
                key = (ikey, event.thread_id)
                writer_leaf[key] = (leaf, event.timestamp)

        # Insert success - record which leaf
        elif event.message == MSG_CAS_SUCCESS:
            leaf = event.leaf_ptr
            if leaf:
                insert_success[ikey] = (leaf, event.timestamp, event.thread_id)

        # B-link warning - check if the leaf differs from insert leaf
        elif event.message == MSG_BLINK_WARNING:
            reader_leaf = event.leaf_ptr
            if ikey in insert_success and reader_leaf:
                insert_leaf, insert_time, insert_thread = insert_success[ikey]
                # Check if reader is looking at different leaf
                if reader_leaf != insert_leaf:
                    time_diff = (event.timestamp - insert_time).total_seconds() * 1000
                    findings += 1
                    print(f"âš ï¸  [writer-reader-divergence] ikey={ikey}")
                    print(f"    Writer (insert) leaf: {insert_leaf}")
                    print(f"    Reader (get) leaf:    {reader_leaf}")
                    print(f"    Time since insert:    {time_diff:.3f}ms")
                    print(f"    Insert thread:        {insert_thread}")
                    print(f"    Reader thread:        {event.thread_id}")

                    # Check if there was a split between insert and read
                    next_ptr = event.fields.get("next_ptr")
                    if next_ptr == insert_leaf:
                        print(
                            f"    âš¡ Insert leaf is in B-link chain (next_ptr={next_ptr})"
                        )
                        print(
                            "    This confirms key was inserted before split, but routing goes to wrong leaf"
                        )
                    print()

    if findings == 0:
        print("âœ“ No writer/reader leaf divergence detected.")
        print("\nThis pattern requires specific tracing events:")
        print("  - advance_to_key_by_bound logs with leaf_ptr")
        print("  - B-link warning logs")
        print("\nIf missing, add tracing to these code paths.")
    else:
        print(f"\nTotal: {findings} divergences detected")
        print("\nThis is strong evidence of: keys inserted into leaf that")
        print("becomes unreachable via normal tree traversal after a split.")

    return findings


def anomaly_immediate_verify(events: Sequence[Event]) -> int:
    """
    Detect immediate verification failures.

    Pattern:
    1. Key K is successfully inserted
    2. Immediate get() for key K returns NotFound
    3. This happens within a very short time window (< 1ms typically)

    Based on Analysis.md Â§2.4, these transient failures indicate a
    visibility window where the key is temporarily unfindable.
    """
    print("--- Anomaly: Immediate Verify Failures ---\n")

    # Track insert successes with timestamp
    inserts: dict[
        int, tuple[dt.datetime, str | None, str | None]
    ] = {}  # ikey -> (time, thread, leaf)

    # Track B-link warnings by ikey
    warnings_by_key: dict[int, list[Event]] = defaultdict(list)

    for event in events:
        ikey = event.ikey
        if ikey is None:
            continue

        if event.message == MSG_CAS_SUCCESS:
            inserts[ikey] = (event.timestamp, event.thread_id, event.leaf_ptr)

        elif event.message == MSG_BLINK_WARNING:
            # This is a get() that failed when it shouldn't have
            warnings_by_key[ikey].append(event)

    # Find keys that were inserted but then had B-link warnings
    findings = 0
    immediate_failures: list[tuple[int, float, Event]] = []

    for ikey, warning_events in warnings_by_key.items():
        if ikey not in inserts:
            continue

        insert_time, _, insert_leaf = inserts[ikey]

        for warning in warning_events:
            time_diff_ms = (warning.timestamp - insert_time).total_seconds() * 1000

            # Immediate = within 10ms of insert
            if time_diff_ms > 0 and time_diff_ms < 10:
                findings += 1
                immediate_failures.append((ikey, time_diff_ms, warning))

    if not immediate_failures:
        print("âœ“ No immediate verification failures detected.")
        print("\nKeys were findable immediately after insert.")
        return 0

    # Sort by time delta
    immediate_failures.sort(key=lambda x: x[1])

    print(f"Found {findings} immediate verification failures:\n")

    # Group by thread
    by_thread: Counter[str | None] = Counter()
    for ikey, _, warning in immediate_failures:
        by_thread[warning.thread_id] += 1

    print("--- By Thread ---")
    for thread, count in by_thread.most_common():
        print(f"  {thread or '(none)'}: {count}")

    print("\n--- Failures (sorted by time since insert) ---")
    for ikey, time_diff, warning in immediate_failures[:20]:
        insert_time, _, insert_leaf = inserts[ikey]
        reader_leaf = warning.leaf_ptr
        print(f"  ikey={ikey}: +{time_diff:.3f}ms after insert")
        print(f"    Insert leaf: {insert_leaf}, Reader leaf: {reader_leaf}")
        if insert_leaf != reader_leaf:
            print("    âš ï¸  LEAF MISMATCH - key routed to wrong leaf!")

    print("\n--- Summary ---")
    avg_delay = sum(t for _, t, _ in immediate_failures) / len(immediate_failures)
    max_delay = max(t for _, t, _ in immediate_failures)
    print(f"  Average time to failure: {avg_delay:.3f}ms")
    print(f"  Max time to failure: {max_delay:.3f}ms")

    # Check for leaf mismatches
    mismatches = sum(
        1 for ikey, _, w in immediate_failures if inserts[ikey][2] != w.leaf_ptr
    )
    print(
        f"  Leaf mismatches: {mismatches}/{findings} ({100 * mismatches / findings:.1f}%)"
    )

    return findings


def anomaly_missing_after_success(events: Sequence[Event]) -> int:
    """
    Detect keys that were successfully inserted but are missing from later splits.

    Pattern:
    1. CAS insert succeeds for key K at leaf L
    2. Later split on leaf L does not include key K in either side

    This is direct evidence of the "missing key" bug.
    """
    print("--- Anomaly: Missing After Success ---\n")

    # Track successful inserts per leaf: {leaf -> {ikey -> Event}}
    inserts_per_leaf: dict[str, dict[int, Event]] = defaultdict(dict)

    # Track splits
    splits: list[Event] = []

    for event in events:
        if event.message == MSG_CAS_SUCCESS:
            if event.leaf_ptr and event.ikey is not None:
                inserts_per_leaf[event.leaf_ptr][event.ikey] = event
        elif event.message == MSG_SPLIT_START:
            splits.append(event)

    findings = 0

    for split_event in splits:
        left_ptr = split_event.fields.get("left_ptr")
        if not isinstance(left_ptr, str):
            continue

        # Get keys that should be in this leaf
        expected_keys = set(inserts_per_leaf.get(left_ptr, {}).keys())
        if not expected_keys:
            continue

        # Get keys that appear in split distribution
        found_keys: set[int] = set()
        for i in range(WIDTH_DEFAULT):
            lk = split_event.fields.get(f"left_ikeys_{i}")
            rk = split_event.fields.get(f"right_ikeys_{i}")
            if isinstance(lk, int):
                found_keys.add(lk)
            if isinstance(rk, int):
                found_keys.add(rk)

        missing = expected_keys - found_keys
        if missing:
            findings += len(missing)
            print(f"âš ï¸  [missing-after-success] leaf={left_ptr}")
            print(f"    Split time: {_format_timestamp(split_event.timestamp)}")
            print(f"    Missing keys: {sorted(missing)}")

            for key in sorted(missing)[:3]:  # Show first 3
                insert_event = inserts_per_leaf[left_ptr].get(key)
                if insert_event:
                    print(
                        f"    Key {key} was inserted at {_format_timestamp(insert_event.timestamp)}"
                    )
                    print(
                        f"      slot={insert_event.slot} thread={insert_event.thread_id}"
                    )
            print()

    if findings == 0:
        print("âœ“ No missing-after-success anomalies detected.")
    else:
        print(f"\nTotal: {findings} keys missing after successful insert")

    return findings


def cmd_anomalies(
    events: Sequence[Event],
    *,
    slot_reuse: bool = False,
    slot_steal: bool = False,
    missing: bool = False,
    divergence: bool = False,
    immediate: bool = False,
    run_all: bool = False,
) -> int:
    """Run anomaly detection heuristics."""
    if run_all:
        slot_reuse = slot_steal = missing = divergence = immediate = True

    if not (slot_reuse or slot_steal or missing or divergence or immediate):
        print("No anomaly checks selected.")
        print(
            "Options: --slot-reuse, --slot-steal, --missing, --divergence, --immediate, --all"
        )
        return 2

    total = 0

    if slot_reuse:
        total += anomaly_slot_reuse(events)
        print()

    if slot_steal:
        total += anomaly_slot_steal(events)
        print()

    if missing:
        total += anomaly_missing_after_success(events)
        print()

    if divergence:
        total += anomaly_writer_reader_divergence(events)
        print()

    if immediate:
        total += anomaly_immediate_verify(events)
        print()

    print(f"{'=' * 40}")
    print(f"Total anomalies: {total}")

    return 1 if total > 0 else 0


# ============================================================================
# CSV export
# ============================================================================


def cmd_export_csv(
    events: Sequence[Event], out_path: Path, *, limit: int | None
) -> None:
    """Export matching events to CSV."""
    columns = [
        "timestamp",
        "level",
        "threadId",
        "target",
        "filename",
        "line_number",
        "fields.message",
        "fields.ikey",
        "fields.slot",
        "fields.leaf_ptr",
        "fields.left_ptr",
        "fields.right_ptr",
        "fields.split_pos",
        "fields.split_ikey",
        "fields.old_perm_size",
        "fields.new_perm_size",
    ]

    rows = events if limit is None else events[:limit]
    with out_path.open("w", encoding="utf-8", newline="") as file:
        writer = csv.writer(file)
        writer.writerow(columns)
        for event in rows:
            writer.writerow(
                [
                    _format_timestamp(event.timestamp),
                    event.level,
                    event.thread_id or "",
                    event.target,
                    event.filename,
                    event.line_number if event.line_number is not None else "",
                    event.message,
                    event.fields.get("ikey", ""),
                    event.fields.get("slot", ""),
                    event.fields.get("leaf_ptr", ""),
                    event.fields.get("left_ptr", ""),
                    event.fields.get("right_ptr", ""),
                    event.fields.get("split_pos", ""),
                    event.fields.get("split_ikey", ""),
                    event.fields.get("old_perm_size", ""),
                    event.fields.get("new_perm_size", ""),
                ]
            )
    print(f"Wrote {len(rows)} rows to {out_path}")


# ============================================================================
# Argument parsing
# ============================================================================


def _parse_levels(value: str | None) -> set[str] | None:
    if value is None:
        return None
    return {v.strip().upper() for v in value.split(",") if v.strip()}


def _parse_dt(value: str | None) -> dt.datetime | None:
    if value is None:
        return None
    return (
        _parse_timestamp(value)
        if value.endswith("Z") or "+" in value
        else dt.datetime.fromisoformat(value)
    )


def main(argv: Sequence[str] | None = None) -> int:
    parser = argparse.ArgumentParser(
        description="Analyze masstree formatted tracing logs (logs/masstree_formatted.json).",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  %(prog)s logs/masstree_formatted.json summary
  %(prog)s logs/masstree_formatted.json --ikey 20000 correlate
  %(prog)s logs/masstree_formatted.json missing-keys --expected 0-100
  %(prog)s logs/masstree_formatted.json anomalies --all
  %(prog)s logs/masstree_formatted.json leaf-timeline 0x7913d0000d80
""",
    )
    parser.add_argument(
        "logfile",
        type=Path,
        help="Path to a masstree_formatted.json file (JSON array).",
    )

    # Global filters
    filter_group = parser.add_argument_group("filters")
    filter_group.add_argument(
        "--level", help="Comma-separated levels (e.g. DEBUG,INFO)."
    )
    filter_group.add_argument("--target-re", help="Regex to match `target`.")
    filter_group.add_argument("--message-re", help="Regex to match `fields.message`.")
    filter_group.add_argument("--ikey", type=int, help="Filter by `fields.ikey`.")
    filter_group.add_argument("--leaf", help="Filter by `fields.leaf_ptr` (hex).")
    filter_group.add_argument("--slot", type=int, help="Filter by `fields.slot`.")
    filter_group.add_argument(
        "--thread", dest="thread_id", help="Filter by `threadId`."
    )
    filter_group.add_argument("--since", help="Filter: timestamp >= SINCE (RFC3339).")
    filter_group.add_argument("--until", help="Filter: timestamp <= UNTIL (RFC3339).")

    sub = parser.add_subparsers(dest="cmd", required=True, metavar="COMMAND")

    # Basic commands
    sub.add_parser(
        "summary", help="High-level summary (counts, time range, top messages)."
    )

    stats_p = sub.add_parser("stats", help="Count occurrences of a field path.")
    stats_p.add_argument(
        "--by", required=True, help="Field path (e.g. fields.message)."
    )
    stats_p.add_argument(
        "--top", type=int, default=30, help="Rows to show (default: 30)."
    )

    ev_p = sub.add_parser("events", help="Print matching events.")
    ev_p.add_argument("--limit", type=int, help="Max events to print.")
    ev_p.add_argument("--compact", action="store_true", help="Compact output format.")

    cas_p = sub.add_parser("cas", help="List CAS insert success events.")
    cas_p.add_argument("--limit", type=int, help="Max events to print.")

    splits_p = sub.add_parser(
        "splits", help="List split events with optional slot indices."
    )
    splits_p.add_argument("--limit", type=int, help="Max events to print.")
    splits_p.add_argument(
        "--show-slots",
        action="store_true",
        help="Show slot indices (helps distinguish perm corruption vs overwrite).",
    )

    perm_p = sub.add_parser("perms", help="Decode and validate permutation values.")
    perm_p.add_argument(
        "--width", type=int, default=WIDTH_DEFAULT, help="WIDTH (default: 15)."
    )

    # Advanced commands
    leaf_p = sub.add_parser(
        "leaf-timeline", help="Show complete lifecycle of a specific leaf."
    )
    leaf_p.add_argument("leaf_ptr", help="Leaf pointer (hex, e.g. 0x7913d0000d80).")

    corr_p = sub.add_parser(
        "correlate", help="Trace a key's lifecycle from insert through split/loss."
    )
    corr_p.add_argument(
        "--ikey", type=int, help="Key to trace (overrides global --ikey)."
    )

    miss_p = sub.add_parser(
        "missing-keys", help="Find expected keys missing from successful inserts."
    )
    miss_p.add_argument(
        "--expected",
        required=True,
        help="Expected keys: ranges (0-100) and/or values (20000,30000).",
    )
    miss_p.add_argument(
        "--show-found", action="store_true", help="Also show found keys."
    )

    inter_p = sub.add_parser(
        "interleave", help="Show thread-interleaved operations on contested leaves."
    )
    inter_p.add_argument(
        "--min-threads", type=int, default=2, help="Min threads (default: 2)."
    )
    inter_p.add_argument("--limit", type=int, help="Max leaves to show.")

    casf_p = sub.add_parser(
        "cas-failures", help="Analyze CAS failure patterns and causes."
    )
    casf_p.add_argument("--limit", type=int, help="Sample failures to show.")

    # B-link warnings command
    blink_p = sub.add_parser("blink-warnings", help="List B-link navigation warnings.")
    blink_p.add_argument("--limit", type=int, help="Max warnings to show.")

    anom_p = sub.add_parser("anomalies", help="Run an1maly detection heuristics.")
    anom_p.add_argument(
        "--slot-reuse", action="store_true", help="Detect slot reuse within epoch."
    )
    anom_p.add_argument(
        "--slot-steal", action="store_true", help="Detect slot overwrites."
    )
    anom_p.add_argument(
        "--missing", action="store_true", help="Detect keys missing after success."
    )
    anom_p.add_argument(
        "--divergence",
        action="store_true",
        help="Detect writer/reader leaf divergence.",
    )
    anom_p.add_argument(
        "--immediate",
        action="store_true",
        help="Detect immediate verification failures.",
    )
    anom_p.add_argument("--all", action="store_true", help="Run all anomaly detectors.")

    export_p = sub.add_parser("export-csv", help="Export matching events to CSV.")
    export_p.add_argument("out", type=Path, help="Output CSV path.")
    export_p.add_argument("--limit", type=int, help="Limit rows exported.")

    args = parser.parse_args(argv)

    # Load events
    try:
        events = load_formatted_events(args.logfile)
    except Exception as exc:
        print(f"Failed to load {args.logfile}: {exc}", file=sys.stderr)
        return 2

    # Apply global filters
    level = _parse_levels(args.level)
    target_re = re.compile(args.target_re) if args.target_re else None
    message_re = re.compile(args.message_re) if args.message_re else None
    leaf_ptr = _parse_hex_ptr(args.leaf) if args.leaf else None
    since = _parse_dt(args.since)
    until = _parse_dt(args.until)

    filtered = list(
        iter_filtered(
            events,
            level=level,
            target_re=target_re,
            message_re=message_re,
            ikey=args.ikey,
            leaf_ptr=leaf_ptr,
            slot=args.slot,
            thread_id=args.thread_id,
            since=since,
            until=until,
        )
    )

    # Dispatch to command
    if args.cmd == "summary":
        cmd_summary(filtered)
        return 0

    if args.cmd == "stats":
        cmd_stats(filtered, by=args.by, top=args.top)
        return 0

    if args.cmd == "events":
        cmd_events(filtered, limit=args.limit, compact=args.compact)
        return 0

    if args.cmd == "cas":
        cmd_cas(filtered, limit=args.limit)
        return 0

    if args.cmd == "splits":
        cmd_splits(filtered, limit=args.limit, show_slots=args.show_slots)
        return 0

    if args.cmd == "perms":
        cmd_perms(filtered, width=args.width)
        return 0

    if args.cmd == "leaf-timeline":
        cmd_leaf_timeline(filtered, args.leaf_ptr)
        return 0

    if args.cmd == "correlate":
        # Allow command-specific --ikey to override global
        ikey = getattr(args, "ikey", None) or getattr(args, "ikey", None)
        if ikey is None:
            print("Error: --ikey required for correlate command", file=sys.stderr)
            return 2
        cmd_correlate(events, ikey)  # Use unfiltered events for full context
        return 0

    if args.cmd == "missing-keys":
        expected = _parse_key_ranges(args.expected)
        cmd_missing_keys(events, expected, show_found=args.show_found)  # Unfiltered
        return 0

    if args.cmd == "interleave":
        cmd_interleave(filtered, min_threads=args.min_threads, limit=args.limit)
        return 0

    if args.cmd == "cas-failures":
        cmd_cas_failures(filtered, limit=args.limit)
        return 0

    if args.cmd == "blink-warnings":
        cmd_blink_warnings(filtered, limit=args.limit)
        return 0

    if args.cmd == "anomalies":
        return cmd_anomalies(
            filtered,
            slot_reuse=args.slot_reuse,
            slot_steal=args.slot_steal,
            missing=args.missing,
            divergence=args.divergence,
            immediate=args.immediate,
            run_all=args.all,
        )

    if args.cmd == "export-csv":
        cmd_export_csv(filtered, args.out, limit=args.limit)
        return 0

    print(f"Unhandled command: {args.cmd}", file=sys.stderr)
    return 2


if __name__ == "__main__":
    raise SystemExit(main())
